# dl4mt-material
session2: implementation of attention model with all layer dropout and bleu validation.

session4: implementation of memory enhanced decoder with all layer dropout with bleu validation. 

session6: implementation of attention is all you need: currently not suitable for use(and label smoothing is not implemented yet for some compare)
current version of theano may suffer from some memory-leak issues, making the full-parameter implementation cost too much memory and CPU in building the gradient funtion for deep network.

I'll be grateful if anyone contacts me and offer advice to make this work.